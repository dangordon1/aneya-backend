# Consultation Transcription & Diarization Architecture

## Overview

The production system uses a **batch processing architecture** for speaker diarization:

1. **During Recording**: Real-time transcription via ElevenLabs/Sarvam WebSocket (no speaker labels)
2. **Post-Recording**: Full audio sent for batch diarization (25-50s processing time)
3. **Speaker Identification**:
   - **Manual**: User maps speaker IDs via SpeakerMappingModal
   - **Automatic**: LLM-based identification during consultation summarization

This architecture provides reliable, high-quality speaker diarization and role identification with user control and verification.

---

## Production Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTEND (InputScreen.tsx)                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RECORDING PHASE                                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  User clicks "Start Recording"                                          â”‚
â”‚         â†“                                                                â”‚
â”‚  MediaRecorder starts capturing audio                                   â”‚
â”‚  - Collects 1-second audio blobs into audioChunksRef                    â”‚
â”‚  - Sends to ElevenLabs/Sarvam WebSocket for real-time transcription    â”‚
â”‚         â†“                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚ ElevenLabs Scribe v2 Realtime WebSocket  â”‚                           â”‚
â”‚  â”‚ (or Sarvam for Indian languages)         â”‚                           â”‚
â”‚  â”‚                                           â”‚                           â”‚
â”‚  â”‚ Frontend connects directly to provider   â”‚                           â”‚
â”‚  â”‚ Token obtained from /api/get-*-token     â”‚                           â”‚
â”‚  â”‚                                           â”‚                           â”‚
â”‚  â”‚ Receives: 1-second audio blobs           â”‚                           â”‚
â”‚  â”‚ Returns: Real-time transcript chunks     â”‚                           â”‚
â”‚  â”‚ Latency: ~150-300ms per chunk            â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚         â†“                                                                â”‚
â”‚  Display interim transcript in UI (NO speaker labels yet)               â”‚
â”‚                                                                          â”‚
â”‚  Recording duration: 0s â†’ 130s â†’ ...                                    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POST-RECORDING PHASE (After user stops recording)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  User clicks "Stop Recording"                                           â”‚
â”‚         â†“                                                                â”‚
â”‚  1. Combine all audioChunksRef blobs into single file                   â”‚
â”‚         â†“                                                                â”‚
â”‚  2. Show "Analyzing Speakers..." spinner (user waits here!)             â”‚
â”‚         â†“                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚ POST /api/diarize                        â”‚                           â”‚
â”‚  â”‚ or POST /api/diarize-sarvam              â”‚                           â”‚
â”‚  â”‚                                           â”‚                           â”‚
â”‚  â”‚ Sends: Full audio file (webm)            â”‚                           â”‚
â”‚  â”‚ Language: consultationLanguage           â”‚                           â”‚
â”‚  â”‚                                           â”‚                           â”‚
â”‚  â”‚ BACKEND PROCESSING:                      â”‚                           â”‚
â”‚  â”‚ 1. Convert webm â†’ mp3 (2-5s) â±ï¸         â”‚                           â”‚
â”‚  â”‚ 2. Call ElevenLabs/Sarvam (15-30s) â±ï¸   â”‚                           â”‚
â”‚  â”‚ 3. Group words by speaker (1s)           â”‚                           â”‚
â”‚  â”‚                                           â”‚                           â”‚
â”‚  â”‚ Returns:                                  â”‚                           â”‚
â”‚  â”‚ {                                         â”‚                           â”‚
â”‚  â”‚   segments: [                             â”‚                           â”‚
â”‚  â”‚     {speaker_id, text, start, end}, ...  â”‚                           â”‚
â”‚  â”‚   ],                                      â”‚                           â”‚
â”‚  â”‚   detected_speakers: ["speaker_0", ...], â”‚                           â”‚
â”‚  â”‚   full_transcript: "..."                  â”‚                           â”‚
â”‚  â”‚ }                                         â”‚                           â”‚
â”‚  â”‚                                           â”‚                           â”‚
â”‚  â”‚ Total latency: 25-50 seconds â±ï¸â±ï¸â±ï¸    â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚         â†“                                                                â”‚
â”‚  3. Speaker Identification (two methods):                               â”‚
â”‚     A. SpeakerMappingModal: User manually maps speaker_0 â†’ roles        â”‚
â”‚        - Simple UI to select which speaker is Doctor/Patient            â”‚
â”‚        - User confirms mapping                                          â”‚
â”‚     B. Automatic LLM identification (during summarization):             â”‚
â”‚        - /api/identify-speaker-roles analyzes conversation patterns     â”‚
â”‚        - Claude Haiku identifies roles (1-2s, ~$0.001 cost)             â”‚
â”‚        - Fallback to heuristic method if LLM fails                      â”‚
â”‚         â†“                                                                â”‚
â”‚  4. Apply role mapping and display final transcript                     â”‚
â”‚                                                                          â”‚
â”‚  Total wait time: 25-50 seconds from stop â†’ speaker-labeled transcript  â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Speaker Identification Methods

### Method A: Manual Mapping (SpeakerMappingModal)
**Location**: `aneya-frontend/src/components/SpeakerMappingModal.tsx`

After diarization completes, the user is presented with a modal to map generic speaker IDs (`speaker_0`, `speaker_1`) to roles (Doctor, Patient). This ensures accuracy and gives users control over the mapping.

### Method B: Automatic LLM Identification
**Location**: `aneya-backend/servers/clinical_decision_support/summary.py:170-200`

During consultation summarization, the system automatically identifies speaker roles using the `/api/identify-speaker-roles` endpoint:
- **Model**: Claude Haiku 3.5
- **Latency**: 1-2 seconds
- **Cost**: ~$0.001 per consultation
- **Accuracy**: Analyzes conversation patterns (questions, medical terminology, symptom descriptions)
- **Fallback**: Heuristic-based identification if LLM fails

### When Speaker Identification Happens

**Timeline of speaker identification in the current system:**

| Stage | When | Method | Details |
|-------|------|--------|---------|
| **Recording** | During (0s â†’ end) | None | Real-time transcription has no speaker labels |
| **Diarization** | Post-recording | Batch API | Generic IDs assigned: `speaker_0`, `speaker_1` (25-50s) |
| **Manual Mapping** | After diarization | User interaction | SpeakerMappingModal allows user to map IDs to roles |
| **Summarization** | When generating summary | Automatic LLM | ConsultationSummary.summarize() identifies roles (1-2s) |

**Summary**: Speaker identification happens **POST-RECORDING**, either via manual user mapping or automatic LLM identification during summarization. The system does NOT identify speakers during the live recording.

---

## Experimental Architecture: Chunked Processing (Reference Only)

> **Note**: The following architecture is documented for reference but is NOT currently implemented in production. The batch processing approach above is the stable, production implementation.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTEND (InputScreen.tsx - EXPERIMENTAL)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RECORDING PHASE (Parallel Processing - EXPERIMENTAL)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  User clicks "Start Recording"                                          â”‚
â”‚         â†“                                                                â”‚
â”‚  MediaRecorder starts capturing audio                                   â”‚
â”‚  - Collects 1-second blobs into audioChunksRef                          â”‚
â”‚  - Real-time transcription via ElevenLabs/Sarvam WebSocket             â”‚
â”‚         â†“                                                                â”‚
â”‚  Every 30 seconds during recording:                                     â”‚
â”‚         â†“                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ CHUNK 0 (t=30s): Extract 0-30s audio                    â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ POST /api/diarize-chunk                                  â”‚            â”‚
â”‚  â”‚ Sends: 30s audio blob, overlap metadata                 â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ BACKEND: Diarize in parallel (2-3s) â±ï¸                  â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ Returns: segments + overlap_stats                        â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ Frontend receives diarized segments                      â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ TRIGGER: POST /api/identify-speaker-roles (background)   â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ BACKEND: Claude Haiku analyzes first 20 segments (1.3s)  â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ Returns: {speaker_0: "Doctor", speaker_1: "Patient"}     â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ Store role mapping in state                              â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ UI UPDATE: Show "Doctor" and "Patient" labels!           â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                          â”‚
â”‚  At t=60s:                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ CHUNK 1 (t=60s): Extract 25-60s audio (5s overlap)      â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ POST /api/diarize-chunk (parallel request)               â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ BACKEND: Diarize (2-3s) â±ï¸                               â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ Frontend: Match speakers using overlap (25-30s region)   â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ Remap speaker_1 â†’ speaker_0 (based on overlap stats)     â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ Apply role mapping: speaker_0 â†’ "Doctor"                 â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ Merge new segments into transcript                       â”‚            â”‚
â”‚  â”‚         â†“                                                 â”‚            â”‚
â”‚  â”‚ UI UPDATE: Append new Doctor/Patient segments            â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                          â”‚
â”‚  ... continues every 30s until recording stops                          â”‚
â”‚                                                                          â”‚
â”‚  Recording duration: 0s â†’ 30s â†’ 60s â†’ 90s â†’ 120s â†’ STOP                 â”‚
â”‚  Diarization: Chunk0  Chunk1  Chunk2  Chunk3 â†’ ALL DONE!                â”‚
â”‚                 â†“       â†“       â†“       â†“                                â”‚
â”‚  Role ID:    Doctor/Patient (applied to all chunks)                     â”‚
â”‚                                                                          â”‚
â”‚  User sees progressive transcript with roles DURING recording!          â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POST-RECORDING PHASE (Instant!)                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  User clicks "Stop Recording"                                           â”‚
â”‚         â†“                                                                â”‚
â”‚  IF last chunk not yet processed:                                       â”‚
â”‚    - Extract final chunk (e.g., 115-130s)                               â”‚
â”‚    - POST /api/diarize-chunk                                            â”‚
â”‚    - Match speakers and merge                                           â”‚
â”‚         â†“                                                                â”‚
â”‚  Transcript is already complete! No waiting! âœ…                          â”‚
â”‚         â†“                                                                â”‚
â”‚  User can immediately review and proceed                                â”‚
â”‚                                                                          â”‚
â”‚  Total wait time: 0-3 seconds (only final chunk if needed)              â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## API Endpoints (Production)

#### 1. `/api/get-transcription-token` (GET) & `/api/get-sarvam-token` (GET)
- **Purpose:** Generate temporary tokens for client-side WebSocket connections
- **Providers:**
  - ElevenLabs Scribe v2 Realtime (English/multilingual)
  - Sarvam AI (Indian languages)
- **Returns:**
  - Token for frontend to connect directly to provider's WebSocket
  - Token expires in 15 minutes
- **Used:** Before starting recording
- **Frontend Connection:**
  - Frontend connects directly to provider's WebSocket (wss://...)
  - Sends audio stream (1-second chunks)
  - Receives real-time transcript (no speaker labels)
  - Latency: ~150-300ms per chunk

#### 2. `/api/diarize` (POST)
- **Purpose:** Batch diarization after recording completes
- **Provider:** ElevenLabs Scribe v1
- **Input:** Full audio file (webm/mp3)
- **Processing:**
  - FFmpeg conversion: 2-5s
  - API call: 15-30s
  - Grouping: 1s
- **Output:**
  ```json
  {
    "segments": [
      {"speaker_id": "speaker_0", "text": "...", "start_time": 0.3, "end_time": 2.1},
      ...
    ],
    "detected_speakers": ["speaker_0", "speaker_1"],
    "full_transcript": "..."
  }
  ```
- **Latency:** 25-50 seconds
- **Used:** Currently in production

#### 3. `/api/diarize-sarvam` (POST)
- **Purpose:** Batch diarization for Indian languages
- **Provider:** Sarvam AI
- **Languages:** 11 Indian languages (hi-IN, ta-IN, etc.)
- **Similar latency to ElevenLabs:** 20-40 seconds
- **Used:** For non-English consultations

#### 4. `/api/identify-speaker-roles` (POST)
- **Purpose:** Automatically identify which speaker is doctor vs patient
- **Provider:** Claude Haiku 3.5
- **Used by:** ConsultationSummary module during summarization
- **Input:**
  ```json
  {
    "segments": [
      {"speaker_id": "speaker_0", "text": "How are you feeling?", ...},
      {"speaker_id": "speaker_1", "text": "I have a cough", ...}
    ],
    "language": "en-IN"
  }
  ```
- **Processing:**
  - Analyzes conversation segments to detect patterns
  - Doctors: ask questions, use medical terms, lead conversation
  - Patients: describe symptoms, answer questions
- **Output:**
  ```json
  {
    "success": true,
    "role_mapping": {
      "speaker_0": "Doctor",
      "speaker_1": "Patient"
    },
    "latency_seconds": 1.3,
    "model": "claude-haiku-4-5"
  }
  ```
- **Latency:** 1-2 seconds
- **Cost:** ~$0.001 per consultation (Haiku pricing)
- **Usage:** Called automatically during consultation summarization; falls back to heuristic method on failure

---

## Experimental Endpoints (Not in Production)

#### `/api/diarize-chunk` (POST) - EXPERIMENTAL
- **Purpose:** Incremental diarization during recording
- **Input:**
  ```
  FormData:
    audio: 30-second audio chunk (webm)
    chunk_index: 0, 1, 2, ...
    overlap_start: timestamp where overlap begins
    overlap_end: timestamp where overlap ends
    language: "en-IN"
  ```
- **Processing:**
  - Convert chunk to mp3 (or skip if webm supported)
  - Call ElevenLabs/Sarvam diarization API
  - Calculate overlap statistics for speaker matching
- **Output:**
  ```json
  {
    "success": true,
    "chunk_index": 0,
    "segments": [...],
    "detected_speakers": ["speaker_0", "speaker_1"],
    "start_overlap_stats": {
      "speaker_0": {"duration": 2.4, "words": 15, "segments": 3},
      "speaker_1": {"duration": 2.6, "words": 18, "segments": 2}
    },
    "end_overlap_stats": {...},
    "latency_seconds": 2.3
  }
  ```
- **Latency:** 2-3 seconds per chunk
- **Parallel potential:** All chunks can process simultaneously

## Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RECORDING IN PROGRESS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  Timeline:   0sâ”€â”€â”€â”€â”€30sâ”€â”€â”€â”€â”€60sâ”€â”€â”€â”€â”€90sâ”€â”€â”€â”€120sâ”€â”€â”€â”€130s (STOP)          â”‚
â”‚                      â”‚       â”‚       â”‚       â”‚       â”‚                   â”‚
â”‚  Audio      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                â”‚
â”‚  Capture:           â”‚       â”‚       â”‚       â”‚       â”‚                   â”‚
â”‚                      â”‚       â”‚       â”‚       â”‚       â”‚                   â”‚
â”‚  Chunk              â””â”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  Extraction:          C0      C1      C2      C3      C4                â”‚
â”‚                        â”‚       â”‚       â”‚       â”‚       â”‚                 â”‚
â”‚                        â†“       â†“       â†“       â†“       â†“                 â”‚
â”‚  Diarize           [2.1s]  [2.7s]  [2.5s]  [2.9s]  [1.2s]               â”‚
â”‚  (parallel):          â”‚       â”‚       â”‚       â”‚       â”‚                 â”‚
â”‚                        â†“       â”‚       â”‚       â”‚       â”‚                 â”‚
â”‚  Role ID            [1.3s]    â”‚       â”‚       â”‚       â”‚                 â”‚
â”‚  (Chunk 0 only):      â”‚       â”‚       â”‚       â”‚       â”‚                 â”‚
â”‚                        â†“       â†“       â†“       â†“       â†“                 â”‚
â”‚  Speaker            Match   Match   Match   Match   Final                â”‚
â”‚  Matching:           â”€>C0    C0â”€C1   C1â”€C2   C2â”€C3   merge              â”‚
â”‚                        â”‚       â”‚       â”‚       â”‚       â”‚                 â”‚
â”‚                        â†“       â†“       â†“       â†“       â†“                 â”‚
â”‚  UI Display:      [Doctor]   +11    +12     +9      +3  segments        â”‚
â”‚                   [Patient]  segs   segs    segs    segs                â”‚
â”‚                        â”‚       â”‚       â”‚       â”‚       â”‚                 â”‚
â”‚                   t=33.4s  t=63.4s t=93.4s t=123.9s DONE!               â”‚
â”‚                                                                          â”‚
â”‚  User Experience: "Sees speaker-labeled transcript appear progressively" â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Performance Characteristics (Production System)

| Metric | Production Implementation |
|--------|--------------------------|
| **Real-time transcription latency** | 150-300ms per 1-second chunk |
| **Diarization latency** | 25-50 seconds (post-recording batch) |
| **Speaker identification** | Manual (instant) or LLM (1-2s during summarization) |
| **User wait after recording** | 25-50 seconds for diarization + manual mapping |
| **Accuracy** | High (ElevenLabs/Sarvam diarization + user-confirmed roles) |
| **Cost per consultation** | ~$0.001 (if using LLM identification during summarization) |

---

## Experimental Performance Comparison (Reference Only)

| Metric | Production (Batch) | Experimental (Chunked) | Potential Improvement |
|--------|-------------------|------------------------|----------------------|
| **Time to first speaker labels** | 25-50s after stop | 3-4s after 30s | 8-15x faster |
| **Total processing time (2min recording)** | 25-50s sequential | 11-13s sequential | 2-4x faster |
| **User wait after stop** | 25-50s | 0-3s (final chunk) | 10-20x faster |
| **Speaker role identification** | Manual mapping or LLM | Automatic (Haiku) | Eliminates user step |
| **UI feedback during recording** | None | Progressive updates | New capability |

## Speaker Matching Algorithm

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OVERLAP-BASED SPEAKER MATCHING                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  Chunk 0 (0-30s)           Chunk 1 (25-60s)                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
â”‚  â”‚                   â”‚     â”‚                   â”‚                         â”‚
â”‚  â”‚   speaker_0       â”‚     â”‚   speaker_1       â”‚                         â”‚
â”‚  â”‚   speaker_1       â”‚     â”‚   speaker_0       â”‚                         â”‚
â”‚  â”‚                   â”‚     â”‚                   â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”˜     â””â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                    â”‚         â”‚                                           â”‚
â”‚        OVERLAP     â”‚â•â•â•â•â•â•â•â•â•â”‚  (25-30s shared audio)                    â”‚
â”‚        REGION:     â”‚         â”‚                                           â”‚
â”‚                    â”‚         â”‚                                           â”‚
â”‚  Chunk 0 stats:    â”‚         â”‚  Chunk 1 stats:                           â”‚
â”‚    speaker_0: 2.5s â”‚         â”‚    speaker_1: 2.6s  â† MATCH! (similar)   â”‚
â”‚    speaker_1: 2.4s â”‚         â”‚    speaker_0: 2.4s  â† MATCH! (similar)   â”‚
â”‚                    â”‚         â”‚                                           â”‚
â”‚  Conclusion:       â”‚         â”‚                                           â”‚
â”‚    Chunk1.speaker_1 â†’ maps to â†’ Chunk0.speaker_0                        â”‚
â”‚    Chunk1.speaker_0 â†’ maps to â†’ Chunk0.speaker_1                        â”‚
â”‚                    â”‚         â”‚                                           â”‚
â”‚  Apply mapping:    â”‚         â”‚                                           â”‚
â”‚    All Chunk 1 segments get speaker IDs remapped to match Chunk 0       â”‚
â”‚                    â”‚         â”‚                                           â”‚
â”‚  Similarity Score Calculation:                                           â”‚
â”‚    - Duration similarity: 50% weight                                     â”‚
â”‚    - Word count similarity: 30% weight                                   â”‚
â”‚    - Avg segment length: 20% weight                                      â”‚
â”‚    - Threshold: 70% confidence to accept match                           â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Frontend State Management (New)

```typescript
// New state variables for chunked processing
const [chunkStatuses, setChunkStatuses] = useState<ChunkStatus[]>([]);
const [mergedSegments, setMergedSegments] = useState<DiarizedSegment[]>([]);
const [speakerIdMap, setSpeakerIdMap] = useState<Map<string, string>>(new Map());
const [speakerRoles, setSpeakerRoles] = useState<{[key: string]: string}>({});

interface ChunkStatus {
  index: number;
  status: 'pending' | 'processing' | 'completed' | 'failed';
  startTime: number;
  endTime: number;
  segments?: DiarizedSegment[];
  speakers?: string[];
  error?: string;
}

// Every 30 seconds during recording
useEffect(() => {
  if (isRecording && recordingTime > 0 && recordingTime % 30 === 0) {
    processNextChunk();
  }
}, [isRecording, recordingTime]);
```

## Error Handling & Fallbacks

1. **Chunk processing failure:**
   - Retry failed chunk once
   - If still fails, fall back to batch processing entire recording

2. **Speaker role identification failure:**
   - Fall back to generic labels: "Speaker 1", "Speaker 2"
   - User can manually correct via SpeakerMappingModal

3. **Speaker matching low confidence (<70%):**
   - Still apply mapping but log warning
   - Consider increasing overlap duration to 10s for better accuracy

## Production Implementation Status

âœ… **Production Components (Fully Implemented):**
- `/api/get-transcription-token` - Real-time transcription token generation
- `/api/get-sarvam-token` - Sarvam API token generation
- `/api/diarize` - Post-recording batch diarization (ElevenLabs)
- `/api/diarize-sarvam` - Post-recording batch diarization (Sarvam for Indian languages)
- `/api/identify-speaker-roles` - LLM-based speaker role identification (used during summarization)
- `SpeakerMappingModal` - Manual speaker role mapping UI
- `ConsultationSummary` - Automatic speaker identification during summarization with heuristic fallback

---

## Experimental Components Status (Reference Only)

âœ… **Completed (Experimental):**
- `/api/identify-speaker-roles` endpoint with full context analysis
- Speaker role identification test scripts
- Overlap-based speaker matching algorithm documentation
- Test validation with chunked audio processing

ğŸš§ **Not Implemented (Experimental):**
- `/api/diarize-chunk` endpoint for incremental diarization
- Frontend chunk extraction logic for live recording
- Frontend speaker matching logic between chunks
- Progressive UI updates during recording
- Live integration with SpeakerMappingModal

**Note**: These experimental features are documented for reference but are not planned for immediate production deployment. The current batch processing approach provides reliable, high-quality diarization and speaker identification.
